{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "228a9fdc",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "326af92b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T15:56:30.479232Z",
     "start_time": "2024-01-26T15:56:26.571137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# RDKit for cheminformatics\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# Scikit-learn for machine learning metrics and splitting\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "# PyTorch for neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import l1_loss\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# PyTorch Geometric for graph neural networks\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import SchNet, NNConv, global_mean_pool, GCNConv\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "# from torch_geometric.data import radius_graph\n",
    "\n",
    "# PyTorch Cluster for clustering algorithms\n",
    "from torch_cluster import radius_graph\n",
    "\n",
    "# Matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import matplotlib\n",
    "\n",
    "# Set Matplotlib settings\n",
    "matplotlib.rcParams['font.family'] = 'Liberation Sans'\n",
    "matplotlib.rcParams['font.size'] = 10\n",
    "\n",
    "# CUDA availability and version\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "# print(torch.__version__)\n",
    "# print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733cf48",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cad15935",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T15:56:31.202281Z",
     "start_time": "2024-01-26T15:56:30.480793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>id_lucas</th>\n",
       "      <th>id_therese</th>\n",
       "      <th>smiles</th>\n",
       "      <th>C_Steric_lucas</th>\n",
       "      <th>C_Steric_therese</th>\n",
       "      <th>Steric</th>\n",
       "      <th>LEC</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bromide_0</td>\n",
       "      <td>alle_1</td>\n",
       "      <td>arbr2085</td>\n",
       "      <td>Brc1nc2ccccc2[nH]1</td>\n",
       "      <td>54.71778</td>\n",
       "      <td>54.732343</td>\n",
       "      <td>54.717780</td>\n",
       "      <td>53.296799</td>\n",
       "      <td>53.296799</td>\n",
       "      <td>53.296799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bromide_22</td>\n",
       "      <td>alle_28</td>\n",
       "      <td>arbr2114</td>\n",
       "      <td>Brc1ncccn1</td>\n",
       "      <td>55.62337</td>\n",
       "      <td>55.700242</td>\n",
       "      <td>55.623370</td>\n",
       "      <td>54.943161</td>\n",
       "      <td>54.943161</td>\n",
       "      <td>54.943161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bromide_23</td>\n",
       "      <td>alle_340</td>\n",
       "      <td>arbr4603</td>\n",
       "      <td>COc1ccc2nc(Br)sc2c1</td>\n",
       "      <td>57.84220</td>\n",
       "      <td>57.887626</td>\n",
       "      <td>57.842200</td>\n",
       "      <td>57.374557</td>\n",
       "      <td>57.355922</td>\n",
       "      <td>57.374557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bromide_26</td>\n",
       "      <td>alle_2001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FC(F)(F)c1ccc(OCc2ccccc2)cc1Br</td>\n",
       "      <td>67.52702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.527020</td>\n",
       "      <td>67.146734</td>\n",
       "      <td>67.121692</td>\n",
       "      <td>67.155470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bromide_27</td>\n",
       "      <td>alle_2002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O=C1c2ccccc2-c2ccc(Br)c3cccc1c23</td>\n",
       "      <td>64.68156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.681560</td>\n",
       "      <td>63.260576</td>\n",
       "      <td>63.260576</td>\n",
       "      <td>63.260576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5694</th>\n",
       "      <td>bromide_5695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arbr794</td>\n",
       "      <td>Brc1ccc(-c2cccc3ccccc23)cc1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.837915</td>\n",
       "      <td>60.805468</td>\n",
       "      <td>58.477567</td>\n",
       "      <td>58.477567</td>\n",
       "      <td>58.477567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5695</th>\n",
       "      <td>bromide_5696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arbr795</td>\n",
       "      <td>Brc1ccc(-c2cccc3c2oc2ccccc23)cc1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.630591</td>\n",
       "      <td>60.598725</td>\n",
       "      <td>58.440295</td>\n",
       "      <td>58.440295</td>\n",
       "      <td>58.440295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5696</th>\n",
       "      <td>bromide_5697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arbr798</td>\n",
       "      <td>Brc1ccc(-c2ccccn2)cc1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.662039</td>\n",
       "      <td>60.630085</td>\n",
       "      <td>58.429230</td>\n",
       "      <td>58.422242</td>\n",
       "      <td>58.429230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5697</th>\n",
       "      <td>bromide_5698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arbr799</td>\n",
       "      <td>CC(C)(C)OC(=O)n1cccc1-c1ccc(Br)cc1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.796566</td>\n",
       "      <td>60.764236</td>\n",
       "      <td>58.705856</td>\n",
       "      <td>58.674408</td>\n",
       "      <td>58.705856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5698</th>\n",
       "      <td>bromide_5699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arbr800</td>\n",
       "      <td>Brc1ccc(-c2cccnc2)cc1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.690575</td>\n",
       "      <td>60.658541</td>\n",
       "      <td>58.435054</td>\n",
       "      <td>58.435054</td>\n",
       "      <td>58.435054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5664 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID   id_lucas id_therese                              smiles  \\\n",
       "0        bromide_0     alle_1   arbr2085                  Brc1nc2ccccc2[nH]1   \n",
       "22      bromide_22    alle_28   arbr2114                          Brc1ncccn1   \n",
       "23      bromide_23   alle_340   arbr4603                 COc1ccc2nc(Br)sc2c1   \n",
       "26      bromide_26  alle_2001        NaN      FC(F)(F)c1ccc(OCc2ccccc2)cc1Br   \n",
       "27      bromide_27  alle_2002        NaN    O=C1c2ccccc2-c2ccc(Br)c3cccc1c23   \n",
       "...            ...        ...        ...                                 ...   \n",
       "5694  bromide_5695        NaN    arbr794         Brc1ccc(-c2cccc3ccccc23)cc1   \n",
       "5695  bromide_5696        NaN    arbr795    Brc1ccc(-c2cccc3c2oc2ccccc23)cc1   \n",
       "5696  bromide_5697        NaN    arbr798               Brc1ccc(-c2ccccn2)cc1   \n",
       "5697  bromide_5698        NaN    arbr799  CC(C)(C)OC(=O)n1cccc1-c1ccc(Br)cc1   \n",
       "5698  bromide_5699        NaN    arbr800               Brc1ccc(-c2cccnc2)cc1   \n",
       "\n",
       "      C_Steric_lucas  C_Steric_therese     Steric        LEC        min  \\\n",
       "0           54.71778         54.732343  54.717780  53.296799  53.296799   \n",
       "22          55.62337         55.700242  55.623370  54.943161  54.943161   \n",
       "23          57.84220         57.887626  57.842200  57.374557  57.355922   \n",
       "26          67.52702               NaN  67.527020  67.146734  67.121692   \n",
       "27          64.68156               NaN  64.681560  63.260576  63.260576   \n",
       "...              ...               ...        ...        ...        ...   \n",
       "5694             NaN         60.837915  60.805468  58.477567  58.477567   \n",
       "5695             NaN         60.630591  60.598725  58.440295  58.440295   \n",
       "5696             NaN         60.662039  60.630085  58.429230  58.422242   \n",
       "5697             NaN         60.796566  60.764236  58.705856  58.674408   \n",
       "5698             NaN         60.690575  60.658541  58.435054  58.435054   \n",
       "\n",
       "            max  \n",
       "0     53.296799  \n",
       "22    54.943161  \n",
       "23    57.374557  \n",
       "26    67.155470  \n",
       "27    63.260576  \n",
       "...         ...  \n",
       "5694  58.477567  \n",
       "5695  58.440295  \n",
       "5696  58.429230  \n",
       "5697  58.705856  \n",
       "5698  58.435054  \n",
       "\n",
       "[5664 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####### Load expt and DFT data\n",
    "excel = \"computed_VBur\"\n",
    "bromides = \"Sheet1\"\n",
    "short_bromides = \"Sheet2\"#\"test\"\n",
    "\n",
    "bromide = pd.read_excel(excel+\".xlsx\",bromides,header=0, engine='openpyxl')\n",
    "short = pd.read_excel(excel+\".xlsx\",short_bromides,header=0, engine='openpyxl')\n",
    "\n",
    "bromide.insert(0, 'ID', 'bromide_' + bromide.index.astype(str))\n",
    "short.insert(0, 'ID', 'bromide_' + short.index.astype(str))\n",
    "\n",
    "# display(bromide)\n",
    "\n",
    "clean_df = bromide[bromide[\"Steric\"] > 40] \n",
    "clean_df = clean_df.reset_index(drop=True)\n",
    "\n",
    "bromide_coppermap = pd.read_csv('bromide_coppermap.csv')\n",
    "bromides_test = pd.merge(bromide_coppermap, clean_df, on='id_lucas', how='inner')\n",
    "clean_df = clean_df[~clean_df['id_lucas'].isin(bromides_test['id_lucas'])]\n",
    "\n",
    "# display(bromides_test)\n",
    "display(clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6197c4cc",
   "metadata": {},
   "source": [
    "# PREPARING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1208a81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:15:09] UFFTYPER: Unrecognized atom type: S_6+6 (1)\n"
     ]
    }
   ],
   "source": [
    "def mol_to_graph_data(mol):\n",
    "    mol = mol\n",
    "    # Node features: Atomic numbers, etc.\n",
    "    atomic_nums = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
    "    is_in_ring = [1 if atom.IsInRing() else 0 for atom in mol.GetAtoms()]\n",
    "    is_aromatic = [1 if atom.GetIsAromatic() else 0 for atom in mol.GetAtoms()]\n",
    "    number_bonds = [atom.GetDegree() for atom in mol.GetAtoms()]\n",
    "\n",
    "    # One-hot encoding for number of bonds\n",
    "    max_bonds = 7  # Assuming a maximum of 6 bonds for any atom\n",
    "    one_hot_bonds = [[1 if i == nb else 0 for i in range(max_bonds)] for nb in number_bonds]\n",
    "\n",
    "    # Combine features\n",
    "    features = [[atomic_num] + [is_ring] + [is_arom] + bond_vec \n",
    "                for atomic_num, is_ring, is_arom, bond_vec in zip(atomic_nums, is_in_ring, is_aromatic, one_hot_bonds)]\n",
    "    x = torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "    # Get the adjacency matrix and convert it to edge index\n",
    "    adj_matrix = Chem.GetAdjacencyMatrix(mol)\n",
    "    edge_indices = np.nonzero(adj_matrix)\n",
    "    edge_index = torch.tensor(np.array(edge_indices), dtype=torch.long)\n",
    "\n",
    "    return x, edge_index\n",
    "\n",
    "def graph_data(smiles, target):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    mol = Chem.AddHs(mol)\n",
    "    AllChem.EmbedMolecule(mol)\n",
    "    x, edge_index = mol_to_graph_data(mol)\n",
    "    graph_data = Data(x=x, edge_index=edge_index, y=torch.tensor([target], dtype=torch.float))\n",
    "    return graph_data\n",
    "\n",
    "# Example usage\n",
    "dataframe = clean_df  # Your DataFrame\n",
    "target_column = \"Steric\"  # Column name for the target\n",
    "graph_data_list = [graph_data(smiles, target) for smiles, target in zip(dataframe['smiles'], dataframe[target_column])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e31c12ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save graph_data_list with torch.save\n",
    "# torch.save(graph_data_list, 'graph_data_list_GCN.pt')\n",
    "# # # Load graph_data_list with torch.load\n",
    "# # graph_data_list = torch.load('graph_data_list_GCN.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac0700fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display(graph_data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be80cf12",
   "metadata": {},
   "source": [
    "# NN MODEL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd2290",
   "metadata": {},
   "source": [
    "## EMBEDDING LAYER FOR ATOMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a989fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T04:18:03.236982Z",
     "start_time": "2024-01-27T04:18:03.214620Z"
    }
   },
   "outputs": [],
   "source": [
    "class AtomEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(AtomEmbedding, self).__init__()\n",
    "        # Embedding layers for atomic number, IsInRing, and IsAromatic\n",
    "        self.atomic_num_embedding = nn.Embedding(100, embedding_dim // 2)\n",
    "        self.is_in_ring_embedding = nn.Embedding(2, embedding_dim // 4)\n",
    "        self.is_aromatic_embedding = nn.Embedding(2, embedding_dim // 4)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        # Convert features to long type for embedding\n",
    "        features = features.long()  # Convert features to long type\n",
    "\n",
    "        # Split features\n",
    "        atomic_nums = features[..., 0]\n",
    "        is_in_ring = features[..., 1]\n",
    "        is_aromatic = features[..., 2]\n",
    "        one_hot_bond = features[..., 3:]\n",
    "\n",
    "        # Embed atomic number, IsInRing, and IsAromatic\n",
    "        atomic_num_embedded = self.atomic_num_embedding(atomic_nums)\n",
    "        is_in_ring_embedded = self.is_in_ring_embedding(is_in_ring)\n",
    "        is_aromatic_embedded = self.is_aromatic_embedding(is_aromatic)\n",
    "\n",
    "        # Concatenate embeddings and one-hot bond vector\n",
    "        return torch.cat(\n",
    "            [atomic_num_embedded, is_in_ring_embedded, is_aromatic_embedded, one_hot_bond], \n",
    "            dim=-1\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2053d69e",
   "metadata": {},
   "source": [
    "## MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf940373",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNModified(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_layers, num_filters):\n",
    "        super(GCNModified, self).__init__()\n",
    "\n",
    "        # Calculate the total embedding dimension\n",
    "        total_embedding_dim = (embedding_dim // 2) + (embedding_dim // 4) + (embedding_dim // 4) + 7\n",
    "\n",
    "        # Use AtomEmbedding layer\n",
    "        self.atom_embedding = AtomEmbedding(embedding_dim)\n",
    "\n",
    "        # GCN layers\n",
    "        self.gcn_layers = nn.ModuleList()\n",
    "        self.gcn_layers.append(GCNConv(total_embedding_dim, num_filters))\n",
    "        for _ in range(1, num_layers):\n",
    "            self.gcn_layers.append(GCNConv(num_filters, num_filters))\n",
    "\n",
    "        # Output layer\n",
    "        self.lin = nn.Linear(num_filters, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Embed atomic features\n",
    "        x = self.atom_embedding(x)\n",
    "\n",
    "        # Apply GCN layers\n",
    "        for layer in self.gcn_layers:\n",
    "            x = F.relu(layer(x, edge_index))\n",
    "\n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.lin(x)\n",
    "        return x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95478831",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db0eaf7",
   "metadata": {},
   "source": [
    "## CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd97c8a",
   "metadata": {},
   "source": [
    "### TRAIN-TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00f1f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the random seed for reproducibility\n",
    "# random_seed = 42\n",
    "# np.random.seed(random_seed)\n",
    "# torch.manual_seed(random_seed)\n",
    "\n",
    "# # Your code for data collation and dataset splitting\n",
    "# def collate_data_list(data_list):\n",
    "#     return Batch.from_data_list(data_list)\n",
    "\n",
    "# # Assuming you have a graph dataset called 'graph_data_list' and batch size of 32\n",
    "# train_ratio = 0.8\n",
    "# test_ratio = 0.2\n",
    "\n",
    "# # Calculate the sizes for the training, validation, and test sets\n",
    "# total_size = len(graph_data_list)\n",
    "# train_size = int(train_ratio * total_size)\n",
    "# test_size = total_size - train_size\n",
    "\n",
    "# # Split the dataset into training, validation, and test sets\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "#     graph_data_list, [train_size, test_size], generator=torch.Generator().manual_seed(random_seed)\n",
    "# )\n",
    "\n",
    "# # Create DataLoader for training, validation, and test sets\n",
    "# batch_size = 32\n",
    "# train_loader = GeoDataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = GeoDataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "091ee9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a training/validation set and a test set\n",
    "training_data, test_data = train_test_split(graph_data_list, test_size=0.2, random_state=42)\n",
    "\n",
    "batch_size = 32\n",
    "# Initialize the test DataLoader\n",
    "test_loader = GeoDataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f5032",
   "metadata": {},
   "source": [
    "### HELPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36323a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_training(data_loader, model, device):\n",
    "    model.eval()\n",
    "    total_abs_error = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            data.to(device)\n",
    "            outputs = model(data.x, data.edge_index, data.batch)\n",
    "            abs_error = l1_loss(outputs, data.y, reduction='sum')\n",
    "            total_abs_error += abs_error.item()\n",
    "            total_samples += data.y.size(0)\n",
    "\n",
    "    return total_abs_error / total_samples\n",
    "\n",
    "# # Evaluate the model on the training and validation sets\n",
    "# mae_train = evaluate_model(training_loader, model, device)\n",
    "# mae_val = evaluate_model(test_loader, model, device)\n",
    "\n",
    "# print(f'MAE on Training Set: {mae_train:.2f}')\n",
    "# print(f'MAE on Validation Set: {mae_val:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "236b1eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, training_loader):\n",
    "\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    batches_since_last_printout = 0.\n",
    "    for i, data in enumerate(training_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        # Make predictions for this batch\n",
    "        outputs = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_func(outputs, data.y)        \n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        batches_since_last_printout += 1\n",
    "\n",
    "        n_training_batches = len(training_loader)\n",
    "        if i % 50 == 49 or i + 1 == n_training_batches:  # Check for every 50th batch or the last batch\n",
    "            last_loss = running_loss / batches_since_last_printout  # Use the actual number of batches since the last printout\n",
    "            running_loss = 0.\n",
    "            batches_since_last_printout = 0  # Reset the counter\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62f8a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_CV(i, total_combinations, training_data, n_folds, n_epochs, model, device, batch_size):\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    train_mae_list = []\n",
    "    val_mae_list = []\n",
    "\n",
    "    for fold, (train_indices, val_indices) in enumerate(kf.split(training_data)):\n",
    "        print(f' *Fold {fold + 1}/{n_folds}')\n",
    "        best_vmae = 1_000_000.\n",
    "        best_tmae = 0\n",
    "\n",
    "        # Split the data into training and validation sets\n",
    "        train_dataset = [training_data[i] for i in train_indices]\n",
    "        val_dataset = [training_data[i] for i in val_indices]\n",
    "\n",
    "        # Create data loaders for training and validation sets\n",
    "        training_loader = GeoDataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        validation_loader = GeoDataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Train the model for the current fold\n",
    "        for epoch in range(n_epochs):\n",
    "            model.train(True)\n",
    "            avg_loss = train_one_epoch(epoch, training_loader)\n",
    "\n",
    "            # Evaluate the model on the training and validation sets\n",
    "            mae_train = evaluate_model_CV(training_loader, model, device)\n",
    "            mae_val = evaluate_model_CV(validation_loader, model, device)\n",
    "            \n",
    "            if epoch % 25 == 24:\n",
    "                print(f'   EPOCH {epoch + 1}/{n_epochs}, Fold {fold + 1}/{n_folds}:, Combination {i}/{total_combinations}')\n",
    "                print(f'   MAE train: {mae_train:.3f}; MAE validation {mae_val:.3f}')\n",
    "\n",
    "            # Track best performance, and save the model's state\n",
    "            if mae_val < best_vmae:\n",
    "                best_vmae = mae_val\n",
    "                best_tmae = mae_train\n",
    "\n",
    "        print(f\"  @Best MAE train: {best_tmae:.3f}, validation: {best_vmae:.3f}\")\n",
    "\n",
    "        # Save the best MAE for each fold\n",
    "        train_mae_list.append(best_tmae)\n",
    "        val_mae_list.append(best_vmae)\n",
    "\n",
    "    # Calculate the average MAE across all folds\n",
    "    avg_train_mae = np.mean(train_mae_list)\n",
    "    avg_val_mae = np.mean(val_mae_list)\n",
    "\n",
    "    return avg_train_mae, avg_val_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd1919a",
   "metadata": {},
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7c79371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the filename for the output file\n",
    "# output_filename_all = \"hyperparameters_mae_2DGNN.txt\"\n",
    "\n",
    "# # # Write the header to the file\n",
    "# # with open(output_filename_all, \"w\") as file:\n",
    "# #     file.write(\"embedding_dim, num_rbf, cutoff, num_interactions, train_mae, test_mae\\n\")\n",
    "\n",
    "# ########################################################\n",
    "# embedding_dims = [128, 256] \n",
    "# num_layers = [20]\n",
    "# total_combinations = len(embedding_dims) * len(num_layers)\n",
    "# ########################################################\n",
    "# for i, (embedding_dims, num_layers) in enumerate(itertools.product(embedding_dims, num_layers), start=1):\n",
    "#     print('-' * 40)\n",
    "#     print(f'Combination {i} of {total_combinations}: embedding_dim={embedding_dims}, num_interactions={num_layers}')\n",
    "#     # Instantiate GCN model\n",
    "#     num_filters = (embedding_dims // 2) + (embedding_dims // 4) + (embedding_dims // 4) + 7\n",
    "#     model = GCNModified(embedding_dim, num_layers, num_filters)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "#     loss_func = torch.nn.L1Loss() #MAE\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model.to(device)    \n",
    "#     ##\n",
    "#     n_epochs = 200\n",
    "#     n_folds = 4\n",
    "#     avg_train_mae, avg_val_mae = kfold_CV(i, total_combinations, training_data, n_folds, n_epochs, model, device, batch_size)\n",
    "#     ##\n",
    "#     with open(output_filename_all, \"a\") as file:\n",
    "#         file.write(f\"{embedding_dims}, {num_layers}, {avg_train_mae:.3f}, {avg_val_mae:.3f}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f75f2",
   "metadata": {},
   "source": [
    "## FINAL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32ff77ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNModified(\n",
       "  (atom_embedding): AtomEmbedding(\n",
       "    (atomic_num_embedding): Embedding(100, 128)\n",
       "    (is_in_ring_embedding): Embedding(2, 64)\n",
       "    (is_aromatic_embedding): Embedding(2, 64)\n",
       "  )\n",
       "  (gcn_layers): ModuleList(\n",
       "    (0-4): 5 x GCNConv(263, 263)\n",
       "  )\n",
       "  (lin): Linear(in_features=263, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################################\n",
    "embedding_dims = 256 \n",
    "num_layers = 5\n",
    "########################################################\n",
    "# Instantiate GCN model\n",
    "num_filters = (embedding_dims // 2) + (embedding_dims // 4) + (embedding_dims // 4) + 7\n",
    "model = GCNModified(embedding_dims, num_layers, num_filters)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "loss_func = torch.nn.L1Loss() #MAE\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1399a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "def train_one_epoch_final(epoch_index, training_loader):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    batches_since_last_printout = 0.\n",
    "    for i, data in enumerate(training_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        # Make predictions for this batch\n",
    "        outputs = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_func(outputs, data.y)        \n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        batches_since_last_printout += 1\n",
    "        \n",
    "        n_training_batches = len(training_loader)\n",
    "        if i % 50 == 49 or i + 1 == n_training_batches:  # Check for every 50th batch or the last batch\n",
    "            last_loss = running_loss / batches_since_last_printout  # Use the actual number of batches since the last printout\n",
    "            running_loss = 0.\n",
    "            batches_since_last_printout = 0  # Reset the counter\n",
    "\n",
    "    # Update the learning rate based on the average loss of the epoch\n",
    "    scheduler.step(last_loss)\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70f2d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename_training = \"2DGNN_training_20_FINALTRAINING.txt\"\n",
    "\n",
    "# Write the header to the file\n",
    "with open(output_filename_training, \"w\") as file:\n",
    "    file.write(\"EPOCH, Adjusted MAE, MAE Train, MAE Test, Best MAE Test, epochs_since_improvement\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7389ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EPOCH 2/2000\n",
      "   Adjusted MAE: 10.595; MAE train: 10.595; MAE test: 10.793; Epochs Since Improvement: 1\n",
      "   EPOCH 4/2000\n",
      "   Adjusted MAE: 2.285; MAE train: 2.285; MAE test: 2.294; Epochs Since Improvement: 0\n",
      "   EPOCH 6/2000\n",
      "   Adjusted MAE: 2.414; MAE train: 2.414; MAE test: 2.488; Epochs Since Improvement: 2\n",
      "   EPOCH 8/2000\n",
      "   Adjusted MAE: 14.404; MAE train: 14.404; MAE test: 14.259; Epochs Since Improvement: 4\n",
      "   EPOCH 10/2000\n",
      "   Adjusted MAE: 2.811; MAE train: 2.811; MAE test: 2.727; Epochs Since Improvement: 1\n",
      "   EPOCH 12/2000\n",
      "   Adjusted MAE: 3.443; MAE train: 3.443; MAE test: 3.545; Epochs Since Improvement: 3\n",
      "   EPOCH 14/2000\n",
      "   Adjusted MAE: 3.473; MAE train: 3.473; MAE test: 3.381; Epochs Since Improvement: 1\n",
      "   EPOCH 16/2000\n",
      "   Adjusted MAE: 1.707; MAE train: 1.707; MAE test: 1.705; Epochs Since Improvement: 0\n",
      "   EPOCH 18/2000\n",
      "   Adjusted MAE: 5.141; MAE train: 5.141; MAE test: 5.250; Epochs Since Improvement: 1\n",
      "   EPOCH 20/2000\n",
      "   Adjusted MAE: 1.256; MAE train: 1.256; MAE test: 1.212; Epochs Since Improvement: 0\n",
      "   EPOCH 22/2000\n",
      "   Adjusted MAE: 1.295; MAE train: 1.295; MAE test: 1.250; Epochs Since Improvement: 2\n",
      "   EPOCH 24/2000\n",
      "   Adjusted MAE: 1.068; MAE train: 1.068; MAE test: 1.045; Epochs Since Improvement: 0\n",
      "   EPOCH 26/2000\n",
      "   Adjusted MAE: 1.488; MAE train: 1.488; MAE test: 1.445; Epochs Since Improvement: 2\n",
      "   EPOCH 28/2000\n",
      "   Adjusted MAE: 1.057; MAE train: 1.057; MAE test: 1.048; Epochs Since Improvement: 1\n",
      "   EPOCH 30/2000\n",
      "   Adjusted MAE: 1.744; MAE train: 1.744; MAE test: 1.713; Epochs Since Improvement: 3\n",
      "   EPOCH 32/2000\n",
      "   Adjusted MAE: 0.862; MAE train: 0.862; MAE test: 0.851; Epochs Since Improvement: 0\n",
      "   EPOCH 34/2000\n",
      "   Adjusted MAE: 0.835; MAE train: 0.835; MAE test: 0.860; Epochs Since Improvement: 0\n",
      "Epoch 00036: reducing learning rate of group 0 to 1.0000e-04.\n",
      "   EPOCH 36/2000\n",
      "   Adjusted MAE: 1.391; MAE train: 1.391; MAE test: 1.416; Epochs Since Improvement: 2\n",
      "   EPOCH 38/2000\n",
      "   Adjusted MAE: 1.109; MAE train: 1.109; MAE test: 1.076; Epochs Since Improvement: 4\n",
      "   EPOCH 40/2000\n",
      "   Adjusted MAE: 2.375; MAE train: 2.375; MAE test: 2.379; Epochs Since Improvement: 6\n",
      "   EPOCH 42/2000\n",
      "   Adjusted MAE: 1.640; MAE train: 1.640; MAE test: 1.619; Epochs Since Improvement: 8\n",
      "   EPOCH 44/2000\n",
      "   Adjusted MAE: 1.093; MAE train: 1.093; MAE test: 1.111; Epochs Since Improvement: 10\n",
      "   EPOCH 46/2000\n",
      "   Adjusted MAE: 0.764; MAE train: 0.764; MAE test: 0.784; Epochs Since Improvement: 0\n",
      "   EPOCH 48/2000\n",
      "   Adjusted MAE: 0.867; MAE train: 0.867; MAE test: 0.900; Epochs Since Improvement: 2\n",
      "   EPOCH 50/2000\n",
      "   Adjusted MAE: 1.549; MAE train: 1.549; MAE test: 1.555; Epochs Since Improvement: 4\n",
      "   EPOCH 52/2000\n",
      "   Adjusted MAE: 1.481; MAE train: 1.481; MAE test: 1.479; Epochs Since Improvement: 6\n",
      "   EPOCH 54/2000\n",
      "   Adjusted MAE: 0.898; MAE train: 0.898; MAE test: 0.896; Epochs Since Improvement: 8\n",
      "   EPOCH 56/2000\n",
      "   Adjusted MAE: 0.832; MAE train: 0.832; MAE test: 0.867; Epochs Since Improvement: 10\n",
      "   EPOCH 58/2000\n",
      "   Adjusted MAE: 2.544; MAE train: 2.544; MAE test: 2.535; Epochs Since Improvement: 12\n",
      "   EPOCH 60/2000\n",
      "   Adjusted MAE: 0.975; MAE train: 0.975; MAE test: 1.000; Epochs Since Improvement: 14\n",
      "   EPOCH 62/2000\n",
      "   Adjusted MAE: 0.710; MAE train: 0.710; MAE test: 0.741; Epochs Since Improvement: 0\n",
      "   EPOCH 64/2000\n",
      "   Adjusted MAE: 0.728; MAE train: 0.728; MAE test: 0.791; Epochs Since Improvement: 2\n",
      "   EPOCH 66/2000\n",
      "   Adjusted MAE: 0.782; MAE train: 0.782; MAE test: 0.791; Epochs Since Improvement: 4\n",
      "   EPOCH 68/2000\n",
      "   Adjusted MAE: 0.826; MAE train: 0.826; MAE test: 0.863; Epochs Since Improvement: 6\n",
      "   EPOCH 70/2000\n",
      "   Adjusted MAE: 1.709; MAE train: 1.709; MAE test: 1.792; Epochs Since Improvement: 8\n",
      "   EPOCH 72/2000\n",
      "   Adjusted MAE: 1.900; MAE train: 1.900; MAE test: 1.954; Epochs Since Improvement: 10\n",
      "   EPOCH 74/2000\n",
      "   Adjusted MAE: 0.801; MAE train: 0.801; MAE test: 0.882; Epochs Since Improvement: 1\n",
      "   EPOCH 76/2000\n",
      "   Adjusted MAE: 1.076; MAE train: 1.076; MAE test: 1.154; Epochs Since Improvement: 3\n",
      "   EPOCH 78/2000\n",
      "   Adjusted MAE: 0.914; MAE train: 0.914; MAE test: 0.949; Epochs Since Improvement: 5\n",
      "   EPOCH 80/2000\n",
      "   Adjusted MAE: 0.978; MAE train: 0.978; MAE test: 1.025; Epochs Since Improvement: 7\n",
      "   EPOCH 82/2000\n",
      "   Adjusted MAE: 1.772; MAE train: 1.772; MAE test: 1.781; Epochs Since Improvement: 1\n",
      "   EPOCH 84/2000\n",
      "   Adjusted MAE: 2.030; MAE train: 2.030; MAE test: 2.124; Epochs Since Improvement: 3\n",
      "   EPOCH 86/2000\n",
      "   Adjusted MAE: 1.096; MAE train: 1.096; MAE test: 1.130; Epochs Since Improvement: 5\n",
      "   EPOCH 88/2000\n",
      "   Adjusted MAE: 1.401; MAE train: 1.401; MAE test: 1.436; Epochs Since Improvement: 7\n",
      "   EPOCH 90/2000\n",
      "   Adjusted MAE: 0.619; MAE train: 0.619; MAE test: 0.663; Epochs Since Improvement: 0\n",
      "   EPOCH 92/2000\n",
      "   Adjusted MAE: 0.952; MAE train: 0.952; MAE test: 1.043; Epochs Since Improvement: 2\n",
      "   EPOCH 94/2000\n",
      "   Adjusted MAE: 1.529; MAE train: 1.529; MAE test: 1.589; Epochs Since Improvement: 1\n",
      "   EPOCH 96/2000\n",
      "   Adjusted MAE: 0.721; MAE train: 0.721; MAE test: 0.784; Epochs Since Improvement: 3\n",
      "   EPOCH 98/2000\n",
      "   Adjusted MAE: 0.629; MAE train: 0.629; MAE test: 0.706; Epochs Since Improvement: 5\n",
      "   EPOCH 100/2000\n",
      "   Adjusted MAE: 0.910; MAE train: 0.910; MAE test: 0.927; Epochs Since Improvement: 7\n",
      "   EPOCH 102/2000\n",
      "   Adjusted MAE: 0.690; MAE train: 0.690; MAE test: 0.742; Epochs Since Improvement: 9\n",
      "   EPOCH 104/2000\n",
      "   Adjusted MAE: 1.217; MAE train: 1.217; MAE test: 1.268; Epochs Since Improvement: 11\n",
      "   EPOCH 106/2000\n",
      "   Adjusted MAE: 1.194; MAE train: 1.194; MAE test: 1.240; Epochs Since Improvement: 13\n",
      "   EPOCH 108/2000\n",
      "   Adjusted MAE: 0.562; MAE train: 0.562; MAE test: 0.628; Epochs Since Improvement: 0\n",
      "   EPOCH 110/2000\n",
      "   Adjusted MAE: 1.118; MAE train: 1.118; MAE test: 1.171; Epochs Since Improvement: 2\n",
      "   EPOCH 112/2000\n",
      "   Adjusted MAE: 1.089; MAE train: 1.089; MAE test: 1.128; Epochs Since Improvement: 4\n",
      "   EPOCH 114/2000\n",
      "   Adjusted MAE: 0.877; MAE train: 0.877; MAE test: 0.922; Epochs Since Improvement: 6\n",
      "   EPOCH 116/2000\n",
      "   Adjusted MAE: 0.957; MAE train: 0.957; MAE test: 1.034; Epochs Since Improvement: 8\n",
      "   EPOCH 118/2000\n",
      "   Adjusted MAE: 0.511; MAE train: 0.511; MAE test: 0.574; Epochs Since Improvement: 0\n",
      "   EPOCH 120/2000\n",
      "   Adjusted MAE: 1.306; MAE train: 1.306; MAE test: 1.342; Epochs Since Improvement: 2\n",
      "   EPOCH 122/2000\n",
      "   Adjusted MAE: 1.071; MAE train: 1.071; MAE test: 1.131; Epochs Since Improvement: 4\n",
      "   EPOCH 124/2000\n",
      "   Adjusted MAE: 1.380; MAE train: 1.380; MAE test: 1.414; Epochs Since Improvement: 6\n",
      "   EPOCH 126/2000\n",
      "   Adjusted MAE: 0.811; MAE train: 0.811; MAE test: 0.878; Epochs Since Improvement: 8\n",
      "   EPOCH 128/2000\n",
      "   Adjusted MAE: 0.688; MAE train: 0.688; MAE test: 0.749; Epochs Since Improvement: 10\n",
      "   EPOCH 130/2000\n",
      "   Adjusted MAE: 0.522; MAE train: 0.522; MAE test: 0.592; Epochs Since Improvement: 12\n",
      "Epoch 00132: reducing learning rate of group 0 to 1.0000e-05.\n",
      "   EPOCH 132/2000\n",
      "   Adjusted MAE: 1.310; MAE train: 1.310; MAE test: 1.329; Epochs Since Improvement: 14\n",
      "   EPOCH 134/2000\n",
      "   Adjusted MAE: 0.894; MAE train: 0.894; MAE test: 0.921; Epochs Since Improvement: 16\n",
      "   EPOCH 136/2000\n",
      "   Adjusted MAE: 0.497; MAE train: 0.497; MAE test: 0.555; Epochs Since Improvement: 0\n",
      "   EPOCH 138/2000\n",
      "   Adjusted MAE: 1.996; MAE train: 1.996; MAE test: 2.040; Epochs Since Improvement: 2\n",
      "   EPOCH 140/2000\n",
      "   Adjusted MAE: 0.763; MAE train: 0.763; MAE test: 0.853; Epochs Since Improvement: 1\n",
      "   EPOCH 142/2000\n",
      "   Adjusted MAE: 0.448; MAE train: 0.448; MAE test: 0.519; Epochs Since Improvement: 0\n",
      "Epoch 00143: reducing learning rate of group 0 to 1.0000e-06.\n",
      "   EPOCH 144/2000\n",
      "   Adjusted MAE: 0.528; MAE train: 0.528; MAE test: 0.590; Epochs Since Improvement: 2\n",
      "   EPOCH 146/2000\n",
      "   Adjusted MAE: 0.870; MAE train: 0.870; MAE test: 0.928; Epochs Since Improvement: 4\n",
      "   EPOCH 148/2000\n",
      "   Adjusted MAE: 0.504; MAE train: 0.504; MAE test: 0.585; Epochs Since Improvement: 6\n",
      "   EPOCH 150/2000\n",
      "   Adjusted MAE: 1.059; MAE train: 1.059; MAE test: 1.092; Epochs Since Improvement: 8\n",
      "   EPOCH 152/2000\n",
      "   Adjusted MAE: 0.805; MAE train: 0.805; MAE test: 0.855; Epochs Since Improvement: 10\n",
      "   EPOCH 154/2000\n",
      "   Adjusted MAE: 0.544; MAE train: 0.544; MAE test: 0.631; Epochs Since Improvement: 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00156: reducing learning rate of group 0 to 1.0000e-07.\n",
      "   EPOCH 156/2000\n",
      "   Adjusted MAE: 0.586; MAE train: 0.586; MAE test: 0.653; Epochs Since Improvement: 14\n",
      "   EPOCH 158/2000\n",
      "   Adjusted MAE: 0.976; MAE train: 0.976; MAE test: 1.054; Epochs Since Improvement: 16\n",
      "   EPOCH 160/2000\n",
      "   Adjusted MAE: 0.956; MAE train: 0.956; MAE test: 1.032; Epochs Since Improvement: 18\n",
      "   EPOCH 162/2000\n",
      "   Adjusted MAE: 0.615; MAE train: 0.615; MAE test: 0.696; Epochs Since Improvement: 20\n",
      "   EPOCH 164/2000\n",
      "   Adjusted MAE: 1.044; MAE train: 1.044; MAE test: 1.066; Epochs Since Improvement: 1\n",
      "   EPOCH 166/2000\n",
      "   Adjusted MAE: 0.471; MAE train: 0.456; MAE test: 0.570; Epochs Since Improvement: 3\n",
      "   EPOCH 168/2000\n",
      "   Adjusted MAE: 0.767; MAE train: 0.767; MAE test: 0.813; Epochs Since Improvement: 5\n",
      "   EPOCH 170/2000\n",
      "   Adjusted MAE: 0.648; MAE train: 0.648; MAE test: 0.705; Epochs Since Improvement: 7\n",
      "   EPOCH 172/2000\n",
      "   Adjusted MAE: 1.162; MAE train: 1.162; MAE test: 1.195; Epochs Since Improvement: 9\n",
      "   EPOCH 174/2000\n",
      "   Adjusted MAE: 0.593; MAE train: 0.593; MAE test: 0.679; Epochs Since Improvement: 11\n",
      "   EPOCH 176/2000\n",
      "   Adjusted MAE: 1.050; MAE train: 1.050; MAE test: 1.135; Epochs Since Improvement: 13\n",
      "   EPOCH 178/2000\n",
      "   Adjusted MAE: 0.682; MAE train: 0.682; MAE test: 0.767; Epochs Since Improvement: 15\n",
      "   EPOCH 180/2000\n",
      "   Adjusted MAE: 0.946; MAE train: 0.946; MAE test: 0.990; Epochs Since Improvement: 17\n",
      "Epoch 00181: reducing learning rate of group 0 to 1.0000e-08.\n",
      "   EPOCH 182/2000\n",
      "   Adjusted MAE: 0.529; MAE train: 0.529; MAE test: 0.618; Epochs Since Improvement: 19\n",
      "   EPOCH 184/2000\n",
      "   Adjusted MAE: 0.417; MAE train: 0.417; MAE test: 0.497; Epochs Since Improvement: 0\n",
      "   EPOCH 186/2000\n",
      "   Adjusted MAE: 1.151; MAE train: 1.151; MAE test: 1.208; Epochs Since Improvement: 2\n",
      "   EPOCH 188/2000\n",
      "   Adjusted MAE: 0.407; MAE train: 0.401; MAE test: 0.490; Epochs Since Improvement: 0\n",
      "   EPOCH 190/2000\n",
      "   Adjusted MAE: 0.587; MAE train: 0.587; MAE test: 0.694; Epochs Since Improvement: 1\n",
      "   EPOCH 192/2000\n",
      "   Adjusted MAE: 0.870; MAE train: 0.870; MAE test: 0.917; Epochs Since Improvement: 3\n",
      "   EPOCH 194/2000\n",
      "   Adjusted MAE: 0.720; MAE train: 0.720; MAE test: 0.802; Epochs Since Improvement: 5\n",
      "   EPOCH 196/2000\n",
      "   Adjusted MAE: 0.639; MAE train: 0.639; MAE test: 0.708; Epochs Since Improvement: 7\n",
      "   EPOCH 198/2000\n",
      "   Adjusted MAE: 0.783; MAE train: 0.783; MAE test: 0.862; Epochs Since Improvement: 9\n",
      "   EPOCH 200/2000\n",
      "   Adjusted MAE: 0.523; MAE train: 0.523; MAE test: 0.626; Epochs Since Improvement: 1\n",
      "   EPOCH 202/2000\n",
      "   Adjusted MAE: 0.934; MAE train: 0.934; MAE test: 1.007; Epochs Since Improvement: 3\n",
      "   EPOCH 204/2000\n",
      "   Adjusted MAE: 0.729; MAE train: 0.729; MAE test: 0.802; Epochs Since Improvement: 5\n",
      "   EPOCH 206/2000\n",
      "   Adjusted MAE: 0.434; MAE train: 0.430; MAE test: 0.521; Epochs Since Improvement: 7\n",
      "   EPOCH 208/2000\n",
      "   Adjusted MAE: 0.482; MAE train: 0.468; MAE test: 0.583; Epochs Since Improvement: 9\n",
      "   EPOCH 210/2000\n",
      "   Adjusted MAE: 0.426; MAE train: 0.403; MAE test: 0.514; Epochs Since Improvement: 11\n",
      "   EPOCH 212/2000\n",
      "   Adjusted MAE: 1.147; MAE train: 1.147; MAE test: 1.191; Epochs Since Improvement: 13\n",
      "   EPOCH 214/2000\n",
      "   Adjusted MAE: 0.408; MAE train: 0.403; MAE test: 0.490; Epochs Since Improvement: 15\n",
      "   EPOCH 216/2000\n",
      "   Adjusted MAE: 0.642; MAE train: 0.642; MAE test: 0.728; Epochs Since Improvement: 17\n",
      "   EPOCH 218/2000\n",
      "   Adjusted MAE: 0.593; MAE train: 0.593; MAE test: 0.669; Epochs Since Improvement: 19\n",
      "   EPOCH 220/2000\n",
      "   Adjusted MAE: 0.532; MAE train: 0.532; MAE test: 0.637; Epochs Since Improvement: 1\n",
      "   EPOCH 222/2000\n",
      "   Adjusted MAE: 0.476; MAE train: 0.455; MAE test: 0.578; Epochs Since Improvement: 3\n",
      "   EPOCH 224/2000\n",
      "   Adjusted MAE: 0.475; MAE train: 0.475; MAE test: 0.568; Epochs Since Improvement: 5\n",
      "   EPOCH 226/2000\n",
      "   Adjusted MAE: 0.999; MAE train: 0.999; MAE test: 1.054; Epochs Since Improvement: 7\n",
      "   EPOCH 228/2000\n",
      "   Adjusted MAE: 0.540; MAE train: 0.540; MAE test: 0.625; Epochs Since Improvement: 9\n",
      "   EPOCH 230/2000\n",
      "   Adjusted MAE: 0.381; MAE train: 0.352; MAE test: 0.456; Epochs Since Improvement: 11\n",
      "   EPOCH 232/2000\n",
      "   Adjusted MAE: 0.552; MAE train: 0.552; MAE test: 0.643; Epochs Since Improvement: 13\n",
      "   EPOCH 234/2000\n",
      "   Adjusted MAE: 0.807; MAE train: 0.807; MAE test: 0.916; Epochs Since Improvement: 15\n",
      "   EPOCH 236/2000\n",
      "   Adjusted MAE: 0.645; MAE train: 0.645; MAE test: 0.727; Epochs Since Improvement: 17\n",
      "   EPOCH 238/2000\n",
      "   Adjusted MAE: 0.467; MAE train: 0.465; MAE test: 0.562; Epochs Since Improvement: 19\n",
      "   EPOCH 240/2000\n",
      "   Adjusted MAE: 0.644; MAE train: 0.644; MAE test: 0.732; Epochs Since Improvement: 21\n",
      "   EPOCH 242/2000\n",
      "   Adjusted MAE: 0.408; MAE train: 0.359; MAE test: 0.490; Epochs Since Improvement: 23\n",
      "   EPOCH 244/2000\n",
      "   Adjusted MAE: 1.362; MAE train: 1.362; MAE test: 1.396; Epochs Since Improvement: 25\n",
      "   EPOCH 246/2000\n",
      "   Adjusted MAE: 0.432; MAE train: 0.413; MAE test: 0.522; Epochs Since Improvement: 27\n",
      "   EPOCH 248/2000\n",
      "   Adjusted MAE: 0.462; MAE train: 0.462; MAE test: 0.551; Epochs Since Improvement: 29\n",
      "   EPOCH 250/2000\n",
      "   Adjusted MAE: 0.459; MAE train: 0.422; MAE test: 0.558; Epochs Since Improvement: 31\n",
      "   EPOCH 252/2000\n",
      "   Adjusted MAE: 0.603; MAE train: 0.603; MAE test: 0.692; Epochs Since Improvement: 33\n",
      "   EPOCH 254/2000\n",
      "   Adjusted MAE: 0.925; MAE train: 0.925; MAE test: 1.031; Epochs Since Improvement: 35\n",
      "   EPOCH 256/2000\n",
      "   Adjusted MAE: 0.462; MAE train: 0.444; MAE test: 0.559; Epochs Since Improvement: 37\n",
      "   EPOCH 258/2000\n",
      "   Adjusted MAE: 0.382; MAE train: 0.328; MAE test: 0.452; Epochs Since Improvement: 39\n",
      "   EPOCH 260/2000\n",
      "   Adjusted MAE: 0.732; MAE train: 0.732; MAE test: 0.828; Epochs Since Improvement: 41\n",
      "   EPOCH 262/2000\n",
      "   Adjusted MAE: 0.543; MAE train: 0.543; MAE test: 0.628; Epochs Since Improvement: 43\n",
      "   EPOCH 264/2000\n",
      "   Adjusted MAE: 0.502; MAE train: 0.489; MAE test: 0.609; Epochs Since Improvement: 1\n",
      "   EPOCH 266/2000\n",
      "   Adjusted MAE: 0.362; MAE train: 0.280; MAE test: 0.412; Epochs Since Improvement: 0\n",
      "   EPOCH 268/2000\n",
      "   Adjusted MAE: 0.444; MAE train: 0.426; MAE test: 0.537; Epochs Since Improvement: 2\n",
      "   EPOCH 270/2000\n",
      "   Adjusted MAE: 0.822; MAE train: 0.822; MAE test: 0.885; Epochs Since Improvement: 4\n",
      "   EPOCH 272/2000\n",
      "   Adjusted MAE: 0.495; MAE train: 0.495; MAE test: 0.588; Epochs Since Improvement: 6\n",
      "   EPOCH 274/2000\n",
      "   Adjusted MAE: 0.844; MAE train: 0.844; MAE test: 0.913; Epochs Since Improvement: 8\n",
      "   EPOCH 276/2000\n",
      "   Adjusted MAE: 0.413; MAE train: 0.369; MAE test: 0.497; Epochs Since Improvement: 10\n",
      "   EPOCH 278/2000\n",
      "   Adjusted MAE: 0.646; MAE train: 0.646; MAE test: 0.742; Epochs Since Improvement: 12\n",
      "   EPOCH 280/2000\n",
      "   Adjusted MAE: 0.490; MAE train: 0.490; MAE test: 0.581; Epochs Since Improvement: 14\n",
      "   EPOCH 282/2000\n",
      "   Adjusted MAE: 0.610; MAE train: 0.610; MAE test: 0.732; Epochs Since Improvement: 16\n",
      "   EPOCH 284/2000\n",
      "   Adjusted MAE: 0.922; MAE train: 0.922; MAE test: 1.016; Epochs Since Improvement: 18\n",
      "   EPOCH 286/2000\n",
      "   Adjusted MAE: 0.452; MAE train: 0.406; MAE test: 0.550; Epochs Since Improvement: 20\n",
      "   EPOCH 288/2000\n",
      "   Adjusted MAE: 0.372; MAE train: 0.275; MAE test: 0.419; Epochs Since Improvement: 22\n",
      "   EPOCH 290/2000\n",
      "   Adjusted MAE: 0.382; MAE train: 0.295; MAE test: 0.440; Epochs Since Improvement: 24\n",
      "   EPOCH 292/2000\n",
      "   Adjusted MAE: 0.358; MAE train: 0.308; MAE test: 0.421; Epochs Since Improvement: 0\n",
      "   EPOCH 294/2000\n",
      "   Adjusted MAE: 0.367; MAE train: 0.300; MAE test: 0.427; Epochs Since Improvement: 2\n",
      "   EPOCH 296/2000\n",
      "   Adjusted MAE: 0.455; MAE train: 0.447; MAE test: 0.548; Epochs Since Improvement: 4\n",
      "   EPOCH 298/2000\n",
      "   Adjusted MAE: 0.859; MAE train: 0.859; MAE test: 0.952; Epochs Since Improvement: 6\n",
      "   EPOCH 300/2000\n",
      "   Adjusted MAE: 0.671; MAE train: 0.671; MAE test: 0.775; Epochs Since Improvement: 8\n",
      "   EPOCH 302/2000\n",
      "   Adjusted MAE: 0.397; MAE train: 0.349; MAE test: 0.475; Epochs Since Improvement: 10\n",
      "   EPOCH 304/2000\n",
      "   Adjusted MAE: 0.365; MAE train: 0.272; MAE test: 0.411; Epochs Since Improvement: 12\n",
      "   EPOCH 306/2000\n",
      "   Adjusted MAE: 0.398; MAE train: 0.358; MAE test: 0.477; Epochs Since Improvement: 14\n",
      "   EPOCH 308/2000\n",
      "   Adjusted MAE: 0.929; MAE train: 0.929; MAE test: 0.988; Epochs Since Improvement: 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EPOCH 310/2000\n",
      "   Adjusted MAE: 0.371; MAE train: 0.295; MAE test: 0.429; Epochs Since Improvement: 18\n",
      "   EPOCH 312/2000\n",
      "   Adjusted MAE: 0.807; MAE train: 0.807; MAE test: 0.883; Epochs Since Improvement: 20\n",
      "   EPOCH 314/2000\n",
      "   Adjusted MAE: 0.359; MAE train: 0.306; MAE test: 0.421; Epochs Since Improvement: 22\n",
      "   EPOCH 316/2000\n",
      "   Adjusted MAE: 0.741; MAE train: 0.741; MAE test: 0.822; Epochs Since Improvement: 24\n",
      "   EPOCH 318/2000\n",
      "   Adjusted MAE: 0.401; MAE train: 0.337; MAE test: 0.476; Epochs Since Improvement: 26\n",
      "   EPOCH 320/2000\n",
      "   Adjusted MAE: 0.377; MAE train: 0.301; MAE test: 0.437; Epochs Since Improvement: 28\n",
      "   EPOCH 322/2000\n",
      "   Adjusted MAE: 0.409; MAE train: 0.348; MAE test: 0.488; Epochs Since Improvement: 30\n",
      "   EPOCH 324/2000\n",
      "   Adjusted MAE: 0.656; MAE train: 0.656; MAE test: 0.744; Epochs Since Improvement: 32\n",
      "   EPOCH 326/2000\n",
      "   Adjusted MAE: 0.756; MAE train: 0.756; MAE test: 0.854; Epochs Since Improvement: 34\n",
      "   EPOCH 328/2000\n",
      "   Adjusted MAE: 0.370; MAE train: 0.252; MAE test: 0.402; Epochs Since Improvement: 36\n",
      "   EPOCH 330/2000\n",
      "   Adjusted MAE: 0.471; MAE train: 0.464; MAE test: 0.568; Epochs Since Improvement: 38\n",
      "   EPOCH 332/2000\n",
      "   Adjusted MAE: 0.520; MAE train: 0.511; MAE test: 0.629; Epochs Since Improvement: 40\n",
      "   EPOCH 334/2000\n",
      "   Adjusted MAE: 0.416; MAE train: 0.377; MAE test: 0.501; Epochs Since Improvement: 42\n",
      "   EPOCH 336/2000\n",
      "   Adjusted MAE: 0.463; MAE train: 0.455; MAE test: 0.557; Epochs Since Improvement: 44\n",
      "   EPOCH 338/2000\n",
      "   Adjusted MAE: 0.670; MAE train: 0.670; MAE test: 0.749; Epochs Since Improvement: 46\n",
      "   EPOCH 340/2000\n",
      "   Adjusted MAE: 0.577; MAE train: 0.577; MAE test: 0.684; Epochs Since Improvement: 48\n",
      "   EPOCH 342/2000\n",
      "   Adjusted MAE: 0.378; MAE train: 0.283; MAE test: 0.429; Epochs Since Improvement: 50\n",
      "   EPOCH 344/2000\n",
      "   Adjusted MAE: 0.435; MAE train: 0.411; MAE test: 0.525; Epochs Since Improvement: 52\n",
      "   EPOCH 346/2000\n",
      "   Adjusted MAE: 0.377; MAE train: 0.308; MAE test: 0.440; Epochs Since Improvement: 54\n",
      "   EPOCH 348/2000\n",
      "   Adjusted MAE: 0.388; MAE train: 0.272; MAE test: 0.432; Epochs Since Improvement: 56\n",
      "   EPOCH 350/2000\n",
      "   Adjusted MAE: 0.387; MAE train: 0.292; MAE test: 0.443; Epochs Since Improvement: 58\n",
      "   EPOCH 352/2000\n",
      "   Adjusted MAE: 0.376; MAE train: 0.334; MAE test: 0.448; Epochs Since Improvement: 60\n",
      "   EPOCH 354/2000\n",
      "   Adjusted MAE: 0.423; MAE train: 0.377; MAE test: 0.510; Epochs Since Improvement: 62\n",
      "   EPOCH 356/2000\n",
      "   Adjusted MAE: 0.384; MAE train: 0.272; MAE test: 0.428; Epochs Since Improvement: 64\n",
      "   EPOCH 358/2000\n",
      "   Adjusted MAE: 0.553; MAE train: 0.548; MAE test: 0.667; Epochs Since Improvement: 66\n",
      "   EPOCH 360/2000\n",
      "   Adjusted MAE: 0.389; MAE train: 0.289; MAE test: 0.444; Epochs Since Improvement: 68\n",
      "   EPOCH 362/2000\n",
      "   Adjusted MAE: 0.553; MAE train: 0.553; MAE test: 0.653; Epochs Since Improvement: 70\n",
      "   EPOCH 364/2000\n",
      "   Adjusted MAE: 0.468; MAE train: 0.450; MAE test: 0.567; Epochs Since Improvement: 72\n",
      "   EPOCH 366/2000\n",
      "   Adjusted MAE: 0.409; MAE train: 0.335; MAE test: 0.485; Epochs Since Improvement: 74\n",
      "   EPOCH 368/2000\n",
      "   Adjusted MAE: 0.446; MAE train: 0.416; MAE test: 0.541; Epochs Since Improvement: 76\n",
      "   EPOCH 370/2000\n",
      "   Adjusted MAE: 0.384; MAE train: 0.252; MAE test: 0.413; Epochs Since Improvement: 78\n",
      "   EPOCH 372/2000\n",
      "   Adjusted MAE: 0.391; MAE train: 0.286; MAE test: 0.443; Epochs Since Improvement: 80\n",
      "   EPOCH 374/2000\n",
      "   Adjusted MAE: 1.011; MAE train: 1.011; MAE test: 1.052; Epochs Since Improvement: 82\n",
      "   EPOCH 376/2000\n",
      "   Adjusted MAE: 0.611; MAE train: 0.611; MAE test: 0.712; Epochs Since Improvement: 84\n",
      "   EPOCH 378/2000\n",
      "   Adjusted MAE: 0.698; MAE train: 0.698; MAE test: 0.785; Epochs Since Improvement: 86\n",
      "   EPOCH 380/2000\n",
      "   Adjusted MAE: 0.398; MAE train: 0.308; MAE test: 0.461; Epochs Since Improvement: 88\n",
      "   EPOCH 382/2000\n",
      "   Adjusted MAE: 0.507; MAE train: 0.493; MAE test: 0.615; Epochs Since Improvement: 90\n",
      "   EPOCH 384/2000\n",
      "   Adjusted MAE: 0.405; MAE train: 0.293; MAE test: 0.461; Epochs Since Improvement: 92\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "embedding_dims = 256\n",
    "num_layers = 20\n",
    "num_filters = (embedding_dims // 2) + (embedding_dims // 4) + (embedding_dims // 4) + 7\n",
    "n_epochs = 2000\n",
    "########################################################\n",
    "early_stop_patience = 2000\n",
    "# best_testmae = float('inf')\n",
    "# epochs_since_improvement = 0\n",
    "########################################################\n",
    "# batch_size = 32\n",
    "# Initialize the test DataLoader\n",
    "train_data, val_data = train_test_split(training_data, test_size=0.2, random_state=42)\n",
    "training_loader = GeoDataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = GeoDataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "########################################################\n",
    "model = GCNModified(embedding_dims, num_layers, num_filters)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "loss_func = torch.nn.L1Loss() #MAE\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)    \n",
    "########################################################\n",
    "best_testmae = 1_000_000.\n",
    "best_trainmae = 1_000_000.\n",
    "best_adjusted_mae = 1_000_000.\n",
    "epochs_since_improvement = 0\n",
    "##\n",
    "# overfit_penalty_factor = 0.75  # Adjust this factor to control the penalty strength\n",
    "overfit_penalty_factor = 0.3  # Adjust this value as needed\n",
    "relative_difference_threshold = 0.2  # Define a threshold for acceptable relative difference\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch_final(epoch, training_loader)\n",
    "\n",
    "    # Evaluate the model on the training and validation sets\n",
    "    mae_train = evaluate_model_training(training_loader, model, device)\n",
    "    mae_test = evaluate_model_training(val_loader, model, device)\n",
    "    \n",
    "    overfit_penalty_factor = 0.3  # Adjust this value as needed\n",
    "    relative_difference_threshold = 0.2  # Define a threshold for acceptable relative difference\n",
    "\n",
    "    # Calculate the relative difference between test and train MAE\n",
    "    relative_difference = (mae_test - mae_train) / mae_train\n",
    "\n",
    "    # Apply a penalty if the relative difference exceeds the threshold\n",
    "    if relative_difference > relative_difference_threshold:\n",
    "        overfit_penalty = (relative_difference - relative_difference_threshold) * overfit_penalty_factor\n",
    "    else:\n",
    "        overfit_penalty = 0\n",
    "\n",
    "    adjusted_mae = mae_train + overfit_penalty\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if adjusted_mae < best_adjusted_mae:\n",
    "        best_adjusted_mae = adjusted_mae\n",
    "        best_testmae = mae_test\n",
    "        best_trainmae = mae_train\n",
    "        torch.save(model.state_dict(), 'best_model_2DGNN_20_FINALTRAINING.pth')  # Save the model state\n",
    "        epochs_since_improvement = 0 \n",
    "    else:\n",
    "        epochs_since_improvement += 1\n",
    "\n",
    "    if epoch % 2 == 1:\n",
    "        print(f'   EPOCH {epoch + 1}/{n_epochs}')\n",
    "        print(f'   Adjusted MAE: {adjusted_mae:.3f}; MAE train: {mae_train:.3f}; MAE test: {mae_test:.3f}; Epochs Since Improvement: {epochs_since_improvement}')\n",
    "\n",
    "    if epochs_since_improvement >= early_stop_patience:\n",
    "        print(f\"No improvement in MAE for {early_stop_patience} epochs. Stopping training.\")\n",
    "        break\n",
    "        \n",
    "    with open(output_filename_training, \"a\") as file:\n",
    "        file.write(f\"EPOCH: {epoch}; Adjusted MAE: {adjusted_mae:.3f}; MAE Train: {mae_train:.3f}; MAE Test: {mae_test:.3f}; Best MAE Test: {best_testmae:.3f}; epochs_since_improvement: {epochs_since_improvement}\\n\")      \n",
    "\n",
    "print(f\"@Best MAE train: {best_testmae:.3f}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832b17d3",
   "metadata": {},
   "source": [
    "# METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cf3434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_model(data_loader, model, device):\n",
    "    model.eval()\n",
    "    total_abs_error = 0.0\n",
    "    predictions = []\n",
    "    mol_ids = []\n",
    "    true_values = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            data.to(device)\n",
    "            outputs = model(data.x, data.edge_index, data.batch)\n",
    "            abs_error = l1_loss(outputs, data.y, reduction='sum')\n",
    "            total_abs_error += abs_error.item()\n",
    "            \n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            true_values.extend(data.y.cpu().numpy())\n",
    "            \n",
    "    r2 = r2_score(true_values, predictions)\n",
    "    \n",
    "    return total_abs_error / len(true_values), r2\n",
    "\n",
    "# # Evaluate the model on the training and validation sets\n",
    "# mae_train, r2_train = evaluate_model(training_loader, model, device)\n",
    "# mae_test, r2_test = evaluate_model(test_loader, model, device)\n",
    "\n",
    "# print(f'MAE on Training Set: {mae_train:.2f}')\n",
    "# print(f'R2 Training Set: {r2_train:.2f}')\n",
    "# print(f'MAE on Validation Set: {mae_test:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d4aa623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on Training Set: 0.354\n",
      "R2 Training Set: 0.95\n",
      "MAE on Validation Set: 0.451\n",
      "R2 Validation Set: 0.92\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "embedding_dims = 256\n",
    "num_layers = 20\n",
    "num_filters = (embedding_dims // 2) + (embedding_dims // 4) + (embedding_dims // 4) + 7\n",
    "#####\n",
    "\n",
    "# Initialize the model with the same architecture as the saved model\n",
    "model = GCNModified(embedding_dims, num_layers, num_filters)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)    \n",
    "training_loader = GeoDataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('best_model_2DGNN_20.pth'))\n",
    "# torch.save(model.state_dict(), 'best_model_256_20_2DGNN.pth')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Evaluate the model on the training and validation sets\n",
    "mae_train, r2_train = evaluate_model(training_loader, model, device)\n",
    "mae_test, r2_test = evaluate_model(test_loader, model, device)\n",
    "\n",
    "print(f'MAE on Training Set: {mae_train:.3f}')\n",
    "print(f'R2 Training Set: {r2_train:.2f}')\n",
    "print(f'MAE on Validation Set: {mae_test:.3f}')\n",
    "print(f'R2 Validation Set: {r2_test:.2f}')\n",
    "\n",
    "# MAE on Training Set: 0.33\n",
    "# R2 Training Set: 0.96\n",
    "# MAE on Validation Set: 0.45\n",
    "# R2 Validation Set: 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ab4bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(data_loader, model, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            data = data.to(device)\n",
    "            outputs = model(data.x, data.edge_index, data.batch)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(data.y.cpu().numpy())\n",
    "    \n",
    "    return actuals, predictions\n",
    "\n",
    "train_actuals, train_predictions = make_predictions(training_loader, model, device)\n",
    "test_actuals, test_predictions = make_predictions(test_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7e3c7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on Training Set: 0.354\n",
      "R2 Training Set: 0.95\n",
      "MAE on Validation Set: 0.451\n",
      "R2 Validation Set: 0.92\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming min_y and max_y are set to the desired range\n",
    "min_y = 50\n",
    "max_y = 80\n",
    "\n",
    "# Generate the ticks for both x and y axes\n",
    "ticks = np.arange(min_y, max_y + 1, 5)\n",
    "\n",
    "# Plot the predicted vs. actual values\n",
    "print(f'MAE on Training Set: {mae_train:.3f}')\n",
    "print(f'R2 Training Set: {r2_train:.2f}')\n",
    "print(f'MAE on Validation Set: {mae_test:.3f}')\n",
    "print(f'R2 Validation Set: {r2_test:.2f}')\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "\n",
    "plt.scatter(train_actuals, train_predictions, marker='o', s=3, color='#FC766AFF', label='Train', alpha=0.5)\n",
    "plt.scatter(test_actuals, test_predictions, marker='o', s=3, color='#42C2A8', label='Test', alpha=0.5)\n",
    "plt.plot([min_y, max_y], [min_y, max_y], color='black', alpha=0.2, linestyle=':', label='Ideal Line')\n",
    "\n",
    "plt.xlabel(\"DFT Steric\")\n",
    "plt.ylabel(\"3D-MPNN Steric\")\n",
    "plt.yticks(ticks)\n",
    "plt.xticks(ticks)\n",
    "\n",
    "plt.legend(fontsize='small', frameon=False, handletextpad=0.5)\n",
    "# plt.legend(loc='upper left', fontsize='small', frameon=False, handletextpad=0.5)\n",
    "# plt.savefig('MPNN_128x128_200.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b7954f",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab7718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ggn_3",
   "language": "python",
   "name": "gnn_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
